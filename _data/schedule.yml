- Title: Breakfast 
  Time: 08:15M - 08:20AM
  Whole_line: true

- Title: Opening Speech
  Time: 08:15AM - 08:20AM
  Whole_line: true  

- Title: (<b>KeyNote Talk</b>) Deploying efficient translation at every level of the stack 
  Time: 08:20AM - 08:45AM
  Presenter: Kenneth Heafield
  Is_paper: false
  Whole_line: false
  Bio: Kenneth Heafield is the founder of Efficient Translation Limited and left the University of Edinburgh last week, where he was an Associate Professor.  He trained a large language model on a trillion tokens in 2013 before it was cool.  Then he ran EU projects ParaCrawl to gather data from the web, Bergamot to make fast translation for browsers, and HPLT to make language models large again.
  Abstract: Practical efficient neural networks combine several optimizations ranging from assembly code to network structure. Yet most papers about optimization start with an unoptimized baseline, omitting comparison even with simple methods like using a smaller network. Shared tasks force a different mentality, where each idea has to prove its worth against a highly optimized baseline. This informs our work on fast and small machine translation with latency under 20 ms for an average sentence. The models are now deployed in Firefox.

- Title: (<b>KeyNote Talk</b>) Simple and efficient self-training approaches for speech recognition 
  Time: 08:45AM - 09:30AM
  Presenter: Samy Bengio <br> Tatiana Likhomanenko
  Is_paper: false
  Whole_line: false
  Bio: <b>Samy Bengio</b> is a senior director of machine learning research at Apple since 2021. Before that, he was a distinguished scientist at Google Research since 2007 where he was heading part of the Google Brain team, and at IDIAP in the early 2000s where he co-wrote the well-known open-source Torch machine learning library. His research interests span many areas of machine learning such as deep architectures, representation learning, vision and language processing and more recently, reasoning. He is action editor of the Journal of Machine Learning Research and on the board of the NeurIPS foundation. He was on the editorial board of the Machine Learning Journal, has been program chair (2017) and general chair (2018) of NeurIPS, program chair of ICLR (2015, 2016), general chair of BayLearn (2012-2015), MLMI (2004-2006), as well as NNSP (2002), and on the program committee of several international conferences such as NeurIPS, ICML, ICLR, ECML and IJCAI. More details can be found at http://bengio.abracadoudou.com. <br> <br> <b>Tatiana</b> is a research scientist at the machine learning research team at Apple. Prior to Apple, she was an AI resident and later a postdoctoral research scientist in the speech recognition team, Facebook AI Research. Back in the day, Tatiana received a Ph.D. in mixed type partial differential equations from Moscow State University. For 4 years she worked on applications of machine learning to high energy physics as a researcher in the joint lab at Yandex and CERN, and later at the startup NTechLab, a leader in face recognition. The main focus of her recent research is transformers training and generalization, efficient speech recognition with less supervision and private federated learning. 
  Abstract: Self-training, or pseudo-labeling (PL), algorithms have recently emerged as a powerful strategy for semi-supervised learning in speech recognition in the era of transformers and large scale data. In this talk, we will walk you from the first successful pseudo-labeling algorithms based on teacher-student training, that alternates between training a model and generating pseudo-labels (PLs) with it, to continuous pseudo-labeling algorithms, where PLs are generated in end-to-end manner as training proceeds, improving training speed and the accuracy of the final model. We will discuss different aspects of PL algorithms to make it simple and resource efficient and overall what are the key components of such huge success :what exactly the model learns, how training dynamics changes, how speaker diversity and amount of hours affect training, and how training depends on the language models. Finally, we will show how pseudo-labeling can be used to train a model on a source language with labeled data and to fine-tune it on a target language with only unlabeled data.
  
  
- Title: (<b>Spotlight 1</b>) Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL 
  Time: 09:30AM - 09:36AM
  Presenter: Hao Sun 
  Is_paper: true
  Whole_line: false
  Authors: Hao Sun · Alihan Hüyük · Mihaela van der Schaar
  Abstract: In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization. We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques. We introduce Prompt-OIRL, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data. Such data exists as by-products when diverse prompts are benchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent prompt optimization objective is achieved by first learning an offline reward model. This model can evaluate any query-prompt pairs without accessing LLMs. Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt. Our experimental evaluations across various LLM scales and arithmetic reasoning datasets underscore both the efficacy and economic viability of the proposed approach.
- Title: (<b>Spotlight 2</b>) MultiPrompter:Cooperative Prompt Optimization with Multi-Agent Reinforcement Learning
  Time: 09:36AM - 09:42AM
  Presenter: Dong-Ki Kim 
  Is_paper: true
  Whole_line: false
  Authors: Dong-Ki Kim · Sungryull Sohn · Lajanugen Logeswaran · Dongsub Shim · Honglak Lee
  Abstract: Recently, there has been an increasing interest in automated prompt optimization based on reinforcement learning (RL). This approach offers important advantages, such as generating interpretable prompts and being compatible with black-box foundation models. However, the substantial prompt space size poses challenges for RL-based methods, often leading to suboptimal policy convergence. This paper introduces MultiPrompter, a new framework that views prompt optimization as a cooperative game between prompters who take turns composing a prompt together. Our cooperative prompt optimization effectively reduces the problem size and helps prompters learn optimal prompts. We test our method on the text-to-image task and demonstrate its ability to generate higher-quality images than baselines.
- Title: (<b>Spotlight 3</b>) Decoding Data Quality via Synthetic Corruptions:Embedding-guided Pruning of Code Data 
  Time: 09:42AM - 9:48AM
  Presenter: Yu Yang
  Is_paper: true
  Whole_line: false
  Authors: Yu Yang · Aaditya Singh · Mostafa Elhoushi · Anas Mahmoud · Kushal Tirumala · Fabian Gloeckle · Baptiste Roziere · Carole-Jean Wu · Ari Morcos · Newsha Ardalani
  Abstract: TBDCode datasets, often collected from diverse and uncontrolled sources such as GitHub, potentially suffer from quality issues, thereby affecting the performance and training efficiency of Large Language Models (LLMs) optimized for code generation. Previous studies demonstrated the benefit of using embedding spaces for data pruning, but they mainly focused on duplicate removal or increasing variety, and in other modalities, such as images. Our work focuses on using embeddings to identify and remove low-quality'' code data. First, we explore features oflow-quality'' code in embedding space, through the use of synthetic corruptions. Armed with this knowledge, we devise novel pruning metrics that operate in embedding space to identify and remove low-quality entries in the Stack dataset. We demonstrate the benefits of this synthetic corruption informed pruning (SCIP) approach on the well-established HumanEval and MBPP benchmarks, outperforming existing embedding-based methods. Importantly, we achieve up to a 3% performance improvement over no pruning, thereby showing the promise of insights from synthetic corruptions for data pruning.
- Title: (<b>Spotlight 4</b>) FlashFFTConv:Efficient Convolutions for Long Sequences with Tensor Cores 
  Time: 09:48AM - 9:54AM
  Presenter: Dan Fu
  Is_paper: true
  Whole_line: false
  Authors: Dan Fu · Hermann Kumbong · Eric Nguyen · Christopher Ré
  Abstract: Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time.A major bottleneck is the Fast Fourier Transform (FFT)---which allows long convolutions to run in $O(N \log N)$ time in sequence length $N$ but has poor hardware utilization.In this paper, we study how to optimize the FFT convolution.We find two key bottlenecks:the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy.In response, we propose FlashFFTConv.FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O.FlashFFTConv speeds up exact FFT convolutions by up to 6.54$\times$ over PyTorch and achieves up to 4.4$\times$ speedup end-to-end.Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity and M2-BERT-base to achieve 3.3 points higher GLUE score---matching models with twice the parameter count.
  Title: (<b>Spotlight 5</b>) Ensemble of low-rank adapters for large language model fine-tuning  
  Time: 09:54AM - 10:00AM
  Presenter: Xi Wang
  Is_paper: true
  Whole_line: false
  Authors: Xi Wang · Laurence Aitchison · Maja Rudolph
  Abstract: Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples. One approach commonly used in vision for alleviating this issue is a deep ensemble, which constructs an ensemble by training the same model multiple times using different random initializations. However, there is a huge challenge to ensembling LLMs:the most effective LLMs are very, very large. Keeping a single LLM in memory is already challenging enough:keeping an ensemble of e.g. 5 LLMs in memory is impossible in many settings. To address these issues, we propose an ensemble approach using Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique. Critically, these low-rank adapters represent a very small number of parameters, orders of magnitude less than the underlying pre-trained model. Thus, it is possible to construct large ensembles of LoRA adapters with almost the same computational overhead as using the original model. We find that LoRA ensembles, applied on its own or on top of pre-existing regularization techniques, gives consistent improvements in predictive accuracy and uncertainty quantification.



- Title: Morning Break and <b>Poster Setup</b>
  Time: 10:00AM - 10:30AM
  Whole_line: true


- Title: (KeyNote Talk) TBD
  Time: 10:30AM - 11:00AM
  Presenter: Luke Zettelmoyer
  Is_paper: false
  Whole_line: false
  Bio: TBD
  Abstract: TBD
- Title: (KeyNote Talk) TBD
  Time: 11:00AM - 11:30AM
  Presenter: Sarath Chandar
  Is_paper: false
  Whole_line: false
  Bio: TBD
  Abstract: TBD
  

- Title: (Paper Oral Presentation) TBD
  Time: 11:30AM - 11:40AM
  Presenter: TBD
  Is_paper: true
  Whole_line: false
  Authors: TBD
  Abstract: TBD
- Title: (Paper Oral Presentation) TBD
  Time: 11:40AM - 11:50AM
  Presenter: TBD
  Is_paper: true
  Whole_line: false
  Authors: TBD
  Abstract: TBD
- Title: (Paper Oral Presentation) TBD
  Time: 11:50AM - 12:00PM
  Presenter: TBD
  Is_paper: true
  Whole_line: false
  Authors: TBD
  Abstract: TBD

- Title: Lunch Break
  Time: 12:00PM - 12:45PM
  Whole_line: true
 
- Title: <b>Poster Session I</b>
  Time: 12:45PM - 01:45PM
  Whole_line: true
  
- Title: (KeyNote Talk) TBD
  Time: 01:45PM - 02:15PM
  Presenter: Ali Madani
  Is_paper: false
  Whole_line: false
  Bio: TBD
  Abstract: TBD  
  
- Title: Interactive Industrial Panel
  Time: 02:15PM - 03:00PM
  Presenter: <ul><li>Nazneen Rajan</li><li>Minjia Zhang</li><li>Tanya Roosta</li><li>Tim Dettmers</li></ul>
  Is_paper: false
  Whole_line: false

- Title: Afternoon Break and <b>Poster Session II</b>
  Time: 03:00PM - 03:30PM
  Whole_line: true

- Title: (KeyNote Talk) TBD
  Time: 03:30PM-04:00PM
  Presenter: Tara Sainath
  Is_paper: false
  Whole_line: false
  Bio: TBD
  Abstract: TBD  
  
  
- Title: (KeyNote Talk) TBD
  Time: 04:00PM-04:30PM
  Presenter: Haoli Bai
  Is_paper: false
  Whole_line: false
  Bio: TBD
  Abstract: TBD  
  
- Title: (KeyNote Talk) TBD
  Time: 04:30PM-05:00PM
  Presenter: Kenneth Heafield
  Is_paper: false
  Whole_line: false
  Bio: TBD
  Abstract: TBD  


- Title: Best Paper and Poster Award & Closing
  Time: 05:00PM-05:10PM
  Whole_line: true
